{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import qm\r\n",
    "from entropylab import *\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using the HDF5 backend\r\n",
    "\r\n",
    "The data persistence layer of Entropy supports saving your experiment results to either a SQL based database (most simply to a sqlite db) or to the [HDF5 data format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format). \r\n",
    "\r\n",
    "By default, experiment results are saved to an HDF5 file and additional data is saved to the SQL database. \r\n",
    "\r\n",
    "**Note** this is a change from earlier version behavior where all data was saved to a single SQL file. \r\n",
    "\r\n",
    "This notebook shows this feature, how to to deactivate it and how to migrate your existing databses.\r\n",
    "\r\n",
    "We start by setting up the database files. The data base entry point is calling the `SqlAlchemyDB` function with the db file path. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "db_file='docs_cache/tutorial.db'\r\n",
    "hdf5_file='docs_cache/tutorial.hdf5'\r\n",
    "\r\n",
    "if os.path.exists(db_file):\r\n",
    "  os.remove(db_file)\r\n",
    "  os.remove(hdf5_file)\r\n",
    "  \r\n",
    "db = SqlAlchemyDB(db_file)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running stamp_revision  -> 04ae19b32c08\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This creates two files: one is the `.db` file and a second file with the `.hdf5` extension is created next to it, in the same folder. \r\n",
    "\r\n",
    "You can view the contenst of HDF files directly by using programs such as [hdf5 view](https://www.hdfgroup.org/downloads/hdfview/), or programmatically. In python this can be done with the h5py and PyTables packages, but many programming languages and environments (e.g. MATLAB) have tools to work with HDF5 files. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Turning the feature off\r\n",
    "\r\n",
    "As mentioned above, the HDF5 feature is turned on by default.\r\n",
    "\r\n",
    "You can turn it on by using the `enable_hdf5_storage` flag on the `SqlAlchemyDB` function\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "db=SqlAlchemyDB(path=\"mydb.db\",enable_hdf5_storage=False) #via ctor\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**:Turning this feature off build a DB with results all contained in the SQL file. If you then want to turn it on, you will need to migrate the DB using the upgrade tool we supply (introduced later on this notebook)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Turning HDF5 off using a configuration file\r\n",
    "\r\n",
    "You can turn off the HDF5 feature by default (instead of using a feature flag). \r\n",
    "To do this, you need to create a file called `setting.toml` next to the `.py` file running your entropy graph. \r\n",
    "The `.toml` file should have the following contents:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```toml\r\n",
    "[toggles]\r\n",
    "hdf5_storage = true\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even if the `.toml` file is present, the `SqlAlchemyDB` feature flag value overrides the behavior. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Seeing experiment results as they are saved in HDF5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following example graph generates a results which is then saved to the HDF5 file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "db=SqlAlchemyDB(path=db_file,enable_hdf5_storage=True)\r\n",
    "er = ExperimentResources(db)\r\n",
    "def node_operation():\r\n",
    "    return {'res':np.array([1,2,3,4])}\r\n",
    "\r\n",
    "node1 = PyNode(label=\"first_node\", program=node_operation,output_vars={'res'})\r\n",
    "experiment = Graph(resources=er, graph={node1}, story=\"run_a\") \r\n",
    "handle = experiment.run()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Migration from sqlite"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To migrate an existing DB to HDF5 use the following snippet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "file_to_migrate = 'some_sqlite_db.db'\r\n",
    "results_backend.sqlalchemy.upgrade_db(path=file_to_migrate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 1318a586f31d, Initial migration\n",
      "INFO  [alembic.runtime.migration] Running upgrade 1318a586f31d -> 04ae19b32c08, Add col saved_in_hdf5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The migtation tool copies the data from sqlite file to the hdf5 file. This may take a while to complete on larger database files. Get a coffee while it's working. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9baad5ffd96599aa46e37c5d9a3f057d162236f2ab0db03d284787da31238321"
  },
  "kernelspec": {
   "name": "entropy",
   "display_name": "entropy",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}